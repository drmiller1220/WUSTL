##alpha = 1 is the LASSO
##alpha = 0 is Ridge
### Compare to LASSO
train.model <- cv.glmnet(x = x.train, y = y.train, alpha = 1, nfolds=10, family="binomial", type.measure="mse")
predict.new<- predict(train.model, newx = x.valid, s = train.model$lambda.min)
predict.probs<- 1/(1 + exp(-predict.new))
final.pred<- ifelse(predict.probs>0.5, 1, 0)
lasso_acc <- sum(final.pred == y.valid_num)/length(y.valid_num) # 98% accuracy
### Compare to Ridge
train.model <- cv.glmnet(x = x.train, y = y.train, alpha = 0, nfolds=10, family="binomial", type.measure="mse")
predict.new<- predict(train.model, newx = x.valid, s = train.model$lambda.min)
predixct.probs<- 1/(1 + exp(-predict.new))
final.pred<- ifelse(predict.probs>0.5, 1, 0)
ridge_acc <- sum(final.pred == y.valid_num)/length(y.valid_num) # also 98% accuracy
### both LASSO and ridge outperform naive Bayes
library(stm)
install.packages("stm")
library(stm)
nyt_dtm <- read.csv(("C:\\Users\\drmiller1220\\Documents\\GitHub\\WUSTL\\HW3\\unigram_dm.csv", header = TRUE))
nyt_dtm <- read.csv("C:\\Users\\drmiller1220\\Documents\\GitHub\\WUSTL\\HW3\\unigram_dm.csv", header = TRUE))
nyt_dtm <- read.csv("C:\\Users\\drmiller1220\\Documents\\GitHub\\WUSTL\\HW3\\unigram_dm.csv", header = TRUE)
nyt_dtm_md <- subset(nyt_dtm, select = c(Story.Title, Story.Desk))
nyt_dtm <- subset(nyt_dtm, select = -c(Story.Title, Story.Desk))
stm_list <- {}
class(stm_list)
stm_list <- list()
i=1
each_doc <- matrix(data = rbind(colnames(nyt_dtm), nyt_dtm[i,]))
View(each_doc)
each_doc <- matrix(data = rbind(colnames(nyt_dtm), nyt_dtm[i,]), nrow = 2)
View(each_doc)
colnames(nyt_dtm)
each_doc <- matrix(data = NA, nrow = 2, ncol = 1000)
each_doc[1,] <- colnames(nyt_dtm)
View(each_doc)
each_doc[2,] <- nyt_dtm[i,]
View(nyt_dtm)
each_doc[1]
each_doc[[1]]
each_doc <- matrix(data = NA, nrow = 2, ncol = 1000)
each_doc[2,] <- nyt_dtm[i,]
each_doc[1,] <- colnames(nyt_dtm)
words <- colnames(nyt_dtm)
counts <- nyt_dtm[i,]
each_word <- rbind(words, counts)
View(each_word)
each_word <- as.matrix(rbind(words, counts))
View(each_word)
each_word <- unname(each_word)
View(each_word)
each_word <- each_word[, which(each_word[2,]>0)]
View(each_word)
stm_list <- list()
for(i in 1:dim(nyt_dtm)[1]){
words <- colnames(nyt_dtm)
counts <- nyt_dtm[i,]
each_word <- as.matrix(rbind(words, counts))
each_word <- unname(each_word)
each_word <- each_word[, which(each_word[2,]>0)]
stm_list[i,] <- each_word
}
stm_list <- list()
for(i in 1:dim(nyt_dtm)[1]){
words <- colnames(nyt_dtm)
counts <- nyt_dtm[i,]
each_word <- as.matrix(rbind(words, counts))
each_word <- unname(each_word)
each_word <- each_word[, which(each_word[2,]>0)]
stm_list[[i]] <- each_word
}
colnames(nyt_dtm)
stm_model <- stm(stm_list, vocab = colnames(nyt_dtm), K=8, content = nyt_dtm_md$Story.Desk)
stm_model <- stm(stm_list, vocab = colnames(nyt_dtm), K=8, content = ~nyt_dtm_md$Story.Desk)
colnames(nyt_dtm)
all_words <- colnames(nyt_dtm)
stm_model <- stm(stm_list, vocab = all_words, K=8, content = ~nyt_dtm_md$Story.Desk)
nyt_dtm_md$Story.Des
stm_model <- stm(stm_list, vocab = all_words, K=8)#, content = ~nyt_dtm_md$Story.Desk)
stm_list <- list()
for(i in 1:dim(nyt_dtm)[1]){
words <- as.integer(colnames(nyt_dtm))
counts <- nyt_dtm[i,]
each_word <- as.matrix(rbind(words, counts))
each_word <- unname(each_word)
each_word <- each_word[, which(each_word[2,]>0)]
stm_list[[i]] <- each_word
}
warnings()
c("A","b","c")
as.integer(c("A","b","c"))
as.numeric(c("A","b","c"))
as.numeric(as.factor(c("A","b","c")))
as.numeric(as.factor(c("A","b","c","d","E")))
stm_list <- list()
for(i in 1:dim(nyt_dtm)[1]){
words <- as.integer(as.factor(colnames(nyt_dtm)))
counts <- nyt_dtm[i,]
each_word <- as.matrix(rbind(words, counts))
each_word <- unname(each_word)
each_word <- each_word[, which(each_word[2,]>0)]
stm_list[[i]] <- each_word
}
stm_model <- stm(stm_list, vocab = colnames(nyt_dtm), K=8, content = ~nyt_dtm_md$Story.Desk)
stm_model <- stm(stm_list, vocab = colnames(nyt_dtm), K=8, prevalence = ~nyt_dtm_md$Story.Desk)
labelTopics(stm_model)
lda_model <- stm(stm_list, vocab = colnames(nyt_dtm), K=8)
labelTopics(lda_model)
labelTopics(stm_model)
stm_model$theta
colSums(stm_model$theta)
colSums(lda_model$theta)
stm_model[[1]]
stm_list[[1]]
as.integer(as.factor(colnames(nyt_dtm)))
as.factor(colnames(nyt_dtm))
sort(as.factor(colnames(nyt_dtm)))
as.factor(colnames(nyt_dtm))
as.factor(colnames(nyt_dtm))
nyt_dtm_words <- data.frame(colnames(nyt_dtm), 1:1000)
View(nyt_dtm_words)
setwd("C:\\Users\\drmiller1220\\Documents\\GitHub\\WUSTL\\HW5")
### Problem 1
library(stm)
nyt_dtm <- read.csv("C:\\Users\\drmiller1220\\Documents\\GitHub\\WUSTL\\HW3\\unigram_dm.csv", header = TRUE)
nyt_dtm_md <- subset(nyt_dtm, select = c(Story.Title, Story.Desk))
nyt_dtm <- subset(nyt_dtm, select = -c(Story.Title, Story.Desk))
nyt_dtm_words <- data.frame(colnames(nyt_dtm), 1:1000)
stm_list <- list()
for(i in 1:dim(nyt_dtm)[1]){
words <- nyt_dtm_words[,2]
counts <- nyt_dtm[i,]
each_word <- as.matrix(rbind(words, counts))
each_word <- unname(each_word)
each_word <- each_word[, which(each_word[2,]>0)]
stm_list[[i]] <- each_word
}
stm_model <- stm(stm_list, vocab = colnames(nyt_dtm), K=8, prevalence = ~nyt_dtm_md$Story.Desk)
labelTopics(stm_model)
colSums(stm_model$theta)
lda_model <- stm(stm_list, vocab = colnames(nyt_dtm), K=8)
labelTopics(lda_model)
labelTopics(stm_model)
colSums(stm_model$theta)
labelTopics(lda_model)
colSums(lda_model$theta)
library(glmnet)
mach_dtm <- read.csv("unigram_dm.csv", header = TRUE)
mach_dtm_md <- subset(mach_dtm, select = c(Name))
mach_dtm <- subset(mach_dtm, select = -c(Name))
mach_norm<- mach_dtm
for(z in 1:nrow(mach_norm)){
mach_norm[z,]<- mach_norm[z,]/sum(mach_norm)
}
setwd("C:\\Users\\drmiller1220\\Documents\\GitHub\\WUSTL\\HW3")
library(e1071)
library(glmnet)
nyt_dtm <- read.csv("unigram_dm.csv", header=TRUE)
nyt_dtm_md <- subset(nyt_dtm, select = c(Story.Title, Story.Desk))
nyt_dtm <- subset(nyt_dtm, select = -c(Story.Title, Story.Desk))
nyt_norm<- nyt_dtm
for(z in 1:nrow(nyt_norm)){
nyt_norm[z,]<- nyt_norm[z,]/sum(nyt_norm[z,])
}
obj_func <- function(dtm, kcluster_obj){
obj_func_value <- NULL
for(i in 1:length(kcluster_obj$size)){
for(j in 1:length(kcluster_obj$cluster)){
if(kcluster_obj$cluster[j]==i){
value <- sum((dtm[j,]-kcluster_obj$centers[i,])^2)
obj_func_value <- append(obj_func_value, value)
}
}
}
total_value <- sum(obj_func_value)
return(total_value)
}
sum(nyt_norm)
i=200
k_cluster<- kmeans(nyt_norm, centers = i)
dim(nyt_dtm)
rowSums(nyt_dtm)
library(glmnet)
mach_dtm <- read.csv("unigram_dm.csv", header = TRUE)
mach_dtm_md <- subset(mach_dtm, select = c(Name))
mach_dtm <- subset(mach_dtm, select = -c(Name))
# normalizing counts in dtm
mach_norm<- mach_dtm
for(z in 1:nrow(mach_norm)){
mach_norm[z,]<- mach_norm[z,]/sum(mach_norm)
}
View(mach_dtm)
setwd("C:\\Users\\drmiller1220\\Documents\\GitHub\\WUSTL\\HW5")
mach_dtm <- read.csv("unigram_dm.csv", header = TRUE)
mach_dtm_md <- subset(mach_dtm, select = c(Name))
mach_dtm <- subset(mach_dtm, select = -c(Name))
mach_norm<- mach_dtm
for(z in 1:nrow(mach_norm)){
mach_norm[z,]<- mach_norm[z,]/sum(mach_norm)
}
mach_pca <- prcomp(mach_dtm, scale=TRUE)
plot(mach_pca)
screeplot(mach_pca)
screeplot(mach_pca, type="l")
screeplot(mach_pca, type="l", ylim=c(0,15))
screeplot(mach_pca, type="l", ylim=c(0,12))
screeplot(mach_pca, type="l", ylim=c(0,12), xlim=c(0,20))
screeplot(mach_pca, type="l", ylim=c(0,12))
screeplot(mach_pca, type="l", npcs=20, ylim=c(0,12))
screeplot(mach_pca, type="l", npcs=50, ylim=c(0,12))
biplot(mach_pca, choices = 1:2)
biplot(mach_pca, choices = 1:2 xlabs=FALSE, ylabs=FALSE)
biplot(mach_pca, choices = 1:2, xlabs=FALSE, ylabs=FALSE)
biplot(mach_pca, choices = 1:2, xlabs=NULL, ylabs=NULL)
biplot(mach_pca, choices = 1:2, xlabs=NULL)
biplot(mach_pca, choices = 1:2)
mach_pca$x
mach_pca$rotation
mach_pca$rotation[1,]
mach_pca$rotation["PC1"]
mach_pca$rotation[1]
mach_pca$rotation[1,]
mach_pca$rotation[,1]
sort(mach_pca$rotation[,1])
mach_pca$x
plot(x=mach_pca$x[,1], y=mach_pca$x[,2])
plot(x=mach_pca$x[,1], y=mach_pca$x[,2], pch=NULL)
plot(x=mach_pca$x[,1], y=mach_pca$x[,2], pch="")
mach_pca$x
View(mach_dtm_md)
text(rownames(mach_dtm), 1:188,
label=names(rownames(mach_dtm)), cex=0.6)
setwd("C:\\Users\\drmiller1220\\Documents\\GitHub\\WUSTL\\HW5")
library(glmnet)
mach_dtm <- read.csv("unigram_dm.csv", header = TRUE)
mach_dtm_md <- subset(mach_dtm, select = c(Name))
mach_dtm <- subset(mach_dtm, select = -c(Name))
# normalizing counts in dtm
mach_norm<- mach_dtm
for(z in 1:nrow(mach_norm)){
mach_norm[z,]<- mach_norm[z,]/sum(mach_norm)
}
## section 2
mach_pca <- prcomp(mach_dtm, scale=TRUE)
screeplot(mach_pca, type="l", npcs=50, ylim=c(0,12))
plot(x=mach_pca$x[,1], y=mach_pca$x[,2], pch="")
text(rownames(mach_dtm), 1:188,
label=names(rownames(mach_dtm)), cex=0.6)
rownames(mach_dtm)
plot(x=mach_pca$x[,1], y=mach_pca$x[,2], pch="")
text(x=mach_pca$x[,1], y=mach_pca$x[,2],
label=names(rownames(mach_dtm)), cex=0.6)
names(rownames(mach_dtm))
plot(x=mach_pca$x[,1], y=mach_pca$x[,2], pch="")
text(x=mach_pca$x[,1], y=mach_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
mach_pca$x
mach_pca$rotation
mach_pca$rotation[,1]
sort(mach_pca$rotation[,1])
rev(sort(mach_pca$rotation[,1]))
rev(sort(mach_pca$rotation[,2]))
rev(sort(mach_pca$rotation[,1]))[1:10]
rev(sort(mach_pca$rotation[,2]))[1:10]
rev(sort(mach_pca$rotation[,1]))[1:20] # 20 words with strongest positive relationship with 1st dim
rev(sort(mach_pca$rotation[,2]))[1:20] # 20 words with strongest positive relationship with 2st dim
plot(x=mach_pca$x[,1], y=mach_pca$x[,2], pch="", ylab="Second Dimension", xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_pca$x[,1], y=mach_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
mach_eu <- as.matrix(dist(mach_norm, method="euclidian"))
View(mach_eu)
mach_mds <- cmdscale(mach_eu, k=2)
mach_pca <- prcomp(mach_norm, scale=TRUE)
screeplot(mach_pca, type="l", npcs=50, ylim=c(0,12))
# looking at the plot of variance explained by each additional component, we see a sharp drop from
# 2 to 3 components, and then a gradual decline thereafter.  However, note that even later
# components account for a subjectively larger share of the variance, suggesting that there are many
# sources of nuance among documents that cannot be generalized across documents
plot(x=mach_pca$x[,1], y=mach_pca$x[,2], pch="", ylab="Second Dimension", xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_pca$x[,1], y=mach_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
rev(sort(mach_pca$rotation[,1]))[1:20] # 20 words with strongest positive relationship with 1st dim
rev(sort(mach_pca$rotation[,2]))[1:20] # 20 words with strongest positive relationship with 2st dim
mach_pca <- prcomp(mach_norm, scale=TRUE)
screeplot(mach_pca, type="l", npcs=50, ylim=c(0,12))
mach_pca <- prcomp(mach_dtm, scale=TRUE)
mach_pca <- prcomp(mach_dtm, scale=TRUE)
screeplot(mach_pca, type="l", npcs=50, ylim=c(0,12))
mach_mds_pca <- prcomp(mach_mds, scale=TRUE)
mach_mds_pca <- prcomp(mach_mds, scale=TRUE)
screeplot(mach_mds_pca, type="l", npcs=50, ylim=c(0,12))
screeplot(mach_mds_pca, type="l", npcs=5, ylim=c(0,12))
mach_eu <- as.matrix(dist(mach_dtm, method="euclidian")) #finding euclidian distance of all
mach_mds <- cmdscale(mach_eu, k=2)
mach_mds_pca <- prcomp(mach_mds, scale=TRUE)
screeplot(mach_mds_pca, type="l", npcs=5, ylim=c(0,12))
mach_eu <- as.matrix(dist(mach_dtm, method="euclidian")) #finding euclidian distance of all
mach_mds <- cmdscale(mach_eu, k=2)
mach_mds_pca <- prcomp(mach_mds, scale=TRUE)
screeplot(mach_mds_pca, type="l", npcs=5, ylim=c(0,12))
mach_eu <- as.matrix(dist(mach_norm, method="euclidian")) #finding euclidian distance of all
mach_mds <- cmdscale(mach_eu, k=2)
mach_mds_pca <- prcomp(mach_mds, scale=TRUE)
screeplot(mach_mds_pca, type="l", npcs=5, ylim=c(0,12))
plot(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2], pch="", ylab="Second Dimension",
xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach__mdspca$x[,1], y=mach__mdspca$x[,2],
label=rownames(mach_dtm), cex=0.6)
text(x=mach__mdspca$x[,1], y=mach_mds_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
text(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
rev(sort(mach_mds_pca$rotation[,1]))[1:20] # 20 words with strongest positive relationship with 1st dim
rev(sort(mach_mds_pca$rotation[,2]))[1:20] # 20 words with strongest positive relationship with 2st dim
mach_mds_pca$rotation
mach_mds_pca$x
View(mach_eu)
View(mach_mds)
cor(mach_pca$x[,1], mach_mds_pca$x[,1])
mach_eu <- as.matrix(dist(mach_dtm, method="euclidian")) #finding euclidian distance of all
mach_mds <- cmdscale(mach_eu, k=2)
mach_mds_pca <- prcomp(mach_mds, scale=TRUE)
screeplot(mach_mds_pca, type="l", npcs=5, ylim=c(0,12))
plot(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2], pch="", ylab="Second Dimension",
xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
cor(mach_pca$x[,1], mach_mds_pca$x[,1])
mach_pca <- prcomp(mach_norm, scale=TRUE)
screeplot(mach_pca, type="l", npcs=50, ylim=c(0,12))
plot(x=mach_pca$x[,1], y=mach_pca$x[,2], pch="", ylab="Second Dimension", xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_pca$x[,1], y=mach_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
mach_pca <- prcomp(mach_dtm, scale=TRUE)
screeplot(mach_pca, type="l", npcs=50, ylim=c(0,12))
# looking at the plot of variance explained by each additional component, we see a sharp drop from
# 2 to 3 components, and then a gradual decline thereafter.  However, note that even later
# components account for a subjectively larger share of the variance, suggesting that there are many
# sources of nuance among documents that cannot be generalized across documents
plot(x=mach_pca$x[,1], y=mach_pca$x[,2], pch="", ylab="Second Dimension", xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_pca$x[,1], y=mach_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
rev(sort(mach_pca$rotation[,1]))[1:20] # 20 words with strongest positive relationship with 1st dim
rev(sort(mach_pca$rotation[,2]))[1:20] # 20 words with strongest positive relationship with 2st dim
mach_eu <- as.matrix(dist(mach_dtm, method="euclidian")) #finding euclidian distance of all
mach_mds <- cmdscale(mach_eu, k=2)
mach_mds_pca <- prcomp(mach_mds, scale=TRUE)
screeplot(mach_mds_pca, type="l", npcs=5, ylim=c(0,12))
plot(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2], pch="", ylab="Second Dimension",
xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
cor(mach_pca$x[,1], mach_mds_pca$x[,1])
mach_mds_pca$rotation
mach_pca <- prcomp(mach_norm, scale=TRUE)
screeplot(mach_pca, type="l", npcs=50, ylim=c(0,12))
screeplot(mach_pca, type="l", npcs=50, ylim=c(0,20))
screeplot(mach_pca, type="l", npcs=50, ylim=c(0,30))
screeplot(mach_pca, type="l", npcs=50, ylim=c(0,40))
plot(x=mach_pca$x[,1], y=mach_pca$x[,2], pch="", ylab="Second Dimension", xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_pca$x[,1], y=mach_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
rev(sort(mach_pca$rotation[,1]))[1:20] # 20 words with strongest positive relationship with 1st dim
rev(sort(mach_pca$rotation[,2]))[1:20] # 20 words with strongest positive relationship with 2st dim
mach_eu <- as.matrix(dist(mach_norm, method="euclidian")) #finding euclidian distance of all
mach_mds <- cmdscale(mach_eu, k=2)
mach_mds_pca <- prcomp(mach_mds, scale=TRUE)
screeplot(mach_mds_pca, type="l", npcs=5, ylim=c(0,12))
plot(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2], pch="", ylab="Second Dimension",
xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
cor(mach_pca$x[,1], mach_mds_pca$x[,1]) # correlation between embeddings is -0.77
machDTM <- machDTM/rowSums(as.matrix(mach_dtm))
machDTM <- mach_dtm/rowSums(as.matrix(mach_dtm))
View(machDTM)
View(mach_dtm)
View(mach_norm)
mach_norm <- machDTM
mach_pca <- prcomp(mach_norm, scale=TRUE)
screeplot(mach_pca, type="l", npcs=50, ylim=c(0,40))
# looking at the plot of variance explained by each additional component, we see a sharp drop from
# 2 to 3 components, then from 3 to 4 components, and then a gradual decline thereafter.
# However, note that even later components account for a subjectively larger share of the variance,
# suggesting that there are many sources of nuance among documents that cannot be generalized
# across documents
plot(x=mach_pca$x[,1], y=mach_pca$x[,2], pch="", ylab="Second Dimension", xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_pca$x[,1], y=mach_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
rev(sort(mach_pca$rotation[,1]))[1:20] # 20 words with strongest positive relationship with 1st dim
rev(sort(mach_pca$rotation[,2]))[1:20] # 20 words with strongest positive relationship with 2st dim
# from the plot of the documents against the 1st and 2nd principal components, we see that the
# discriminating power of the 1st or 2nd dimensions is not terribly high; most documents have a near-
# zero value for the first dimension and the second dimension, while only a handful are discriminated
# along the 1st dimension and only 2 are really discriminated along the 2nd dimension.  Most of
# the discriminated documents are the concluding paragraphs, which suggests that the non-concluding
# paragraphs are very different from the concluding ones.  Looking at the most common words in
# these documents, we see that the words characterizing the first dimension words seem to be advice-
# driven, while the words characterizing the second dimension seem to be more steeped in historical
# anecdotes.  These two dimensions do tend to characterize The Prince--advice for statecraft
# accompanied by historical lessons reinforcing that advice.
### Problem 3
mach_eu <- as.matrix(dist(mach_norm, method="euclidian")) #finding euclidian distance of all
# documents
mach_mds <- cmdscale(mach_eu, k=2)
mach_mds_pca <- prcomp(mach_mds, scale=TRUE)
# we do not include a screeplot here because we are limiting ourselves to 2 dimensions
plot(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2], pch="", ylab="Second Dimension",
xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
cor(mach_pca$x[,1], mach_mds_pca$x[,1]) # correlation between embeddings is -0.38
mach_norm<- mach_dtm
for(z in 1:nrow(mach_norm)){
mach_norm[z,]<- mach_norm[z,]/sum(mach_norm[z,])
}
mach_pca <- prcomp(mach_norm, scale=TRUE)
screeplot(mach_pca, type="l", npcs=50, ylim=c(0,40))
mach_pca <- prcomp(mach_norm, scale=TRUE)
screeplot(mach_pca, type="l", npcs=5, ylim=c(0,40))
mach_pca <- prcomp(mach_norm, scale=TRUE)
screeplot(mach_pca, type="l", npcs=10, ylim=c(0,40))
screeplot(mach_pca, type="l", npcs=10, ylim=c(0,15))
screeplot(mach_pca, type="l", npcs=10, ylim=c(0,12))
plot(x=mach_pca$x[,1], y=mach_pca$x[,2], pch="", ylab="Second Dimension", xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_pca$x[,1], y=mach_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
rev(sort(mach_pca$rotation[,1]))[1:20] # 20 words with strongest positive relationship with 1st dim
rev(sort(mach_pca$rotation[,2]))[1:20] # 20 words with strongest positive relationship with 2st dim
mach_eu <- as.matrix(dist(mach_norm, method="euclidian")) #finding euclidian distance of all
mach_mds <- cmdscale(mach_eu, k=2)
mach_mds_pca <- prcomp(mach_mds, scale=TRUE)
plot(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2], pch="", ylab="Second Dimension",
xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
cor(mach_pca$x[,1], mach_mds_pca$x[,1]) # correlation between embeddings is -0.38
screeplot(mach_pca, type="l", npcs=10, ylim=c(0,12))
plot(x=mach_pca$x[,1], y=mach_pca$x[,2], pch="", ylab="Second Dimension", xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_pca$x[,1], y=mach_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
plot(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2], pch="", ylab="Second Dimension",
xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
mach_mds_man <- cmdscale(mach_man, k=2)
mach_man <- as.matrix(dist(mach_norm, method="manhattan")) #finding euclidian distance of all
mach_mds_man <- cmdscale(mach_man, k=2)
mach_mds_man_pca <- prcomp(mach_mds, scale=TRUE)
plot(x=mach_mds_man_pca$x[,1], y=mach_mds_man_pca$x[,2], pch="", ylab="Second Dimension",
xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_mds_man_pca$x[,1], y=mach_mds_man_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
cor(mach_pca$x[,1], mach_mds_man_pca$x[,1])
cor(mach_pca$x[,1], mach_mds_pca$x[,1]) # correlation between embeddings is -0.74, so there is a
mach_mds_man_pca <- prcomp(mach_mds_man, scale=TRUE)
plot(x=mach_mds_man_pca$x[,1], y=mach_mds_man_pca$x[,2], pch="", ylab="Second Dimension",
xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_mds_man_pca$x[,1], y=mach_mds_man_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
cor(mach_pca$x[,1], mach_mds_man_pca$x[,1])
cor(mach_pca$x[,1], mach_mds_man[,1])
cor(mach_pca$x[,1], mach_mds_pca[,1]) # correlation between embeddings is -0.74, so there is a
cor(mach_pca$x[,1], mach_mds[,1]) # correlation between embeddings is -0.74, so there is a
mach_eu <- as.matrix(dist(mach_norm, method="euclidian")) #finding euclidian distance of all
mach_mds <- cmdscale(mach_eu, k=2)
mach_mds_pca <- prcomp(mach_mds, scale=FALSE)
plot(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2], pch="", ylab="Second Dimension",
xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
cor(mach_pca$x[,1], mach_mds_pca$x[,1]) # correlation between embeddings is -0.74, so there is a
mach_man <- as.matrix(dist(mach_norm, method="manhattan")) #finding euclidian distance of all
mach_mds_man <- cmdscale(mach_man, k=2)
mach_mds_man_pca <- prcomp(mach_mds_man, scale=FALSE)
plot(x=mach_mds_man_pca$x[,1], y=mach_mds_man_pca$x[,2], pch="", ylab="Second Dimension",
xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_mds_man_pca$x[,1], y=mach_mds_man_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
cor(mach_pca$x[,1], mach_mds_man_pca$x[,1])
mach_mds_pca <- prcomp(mach_mds, scale=FALSE)
plot(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2], pch="", ylab="Second Dimension",
xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_mds_pca$x[,1], y=mach_mds_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
mach_man <- as.matrix(dist(mach_norm, method="manhattan")) #finding euclidian distance of all
mach_mds_man <- cmdscale(mach_man, k=2)
mach_mds_man_pca <- prcomp(mach_mds_man, scale=FALSE)
plot(x=mach_mds_man_pca$x[,1], y=mach_mds_man_pca$x[,2], pch="", ylab="Second Dimension",
xlab="First Dimension",
main="Plot of First and Second Principal\n Components of The Prince")
text(x=mach_mds_man_pca$x[,1], y=mach_mds_man_pca$x[,2],
label=rownames(mach_dtm), cex=0.6)
